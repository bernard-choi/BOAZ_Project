{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from rnn_model.attention_lstm import bilstm_with_attention\n",
    "from tensorflow.contrib import rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-7a3e4285247c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mWORD_MAX_LEN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m80\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mvocabulary_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontrib\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'word_dict' is not defined"
     ]
    }
   ],
   "source": [
    "num_class=30\n",
    "embedding_size = 256\n",
    "NUM_CLASS = 20\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 1\n",
    "WORD_MAX_LEN = 80\n",
    "vocabulary_size = len(word_dict)\n",
    "from tensorflow.contrib import rnn\n",
    "       \n",
    "      \n",
    "num_class=30\n",
    "embedding_size = 256\n",
    "num_hidden = 256\n",
    "num_layers = 2\n",
    "learning_rate = 1e-3\n",
    "tf.reset_default_graph()\n",
    "x = tf.placeholder(tf.int32, [None, 80], name=\"x\")\n",
    "x_len = tf.reduce_sum(tf.sign(x), 1)\n",
    "y = tf.placeholder(tf.int32, [None], name=\"y\")\n",
    "is_training = tf.placeholder(tf.bool, [], name=\"is_training\")\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "keep_prob = tf.where(is_training, 0.5, 1.0)\n",
    "\n",
    "with tf.name_scope(\"embedding\"):\n",
    "    init_embeddings = tf.random_uniform([vocabulary_size, embedding_size])\n",
    "    embeddings = tf.get_variable(\"embeddings\", initializer=init_embeddings)\n",
    "    x_emb = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "with tf.name_scope(\"birnn\"):\n",
    "    fw_cells = [rnn.BasicLSTMCell(num_hidden) for _ in range(num_layers)]\n",
    "    bw_cells = [rnn.BasicLSTMCell(num_hidden) for _ in range(num_layers)]\n",
    "    fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=keep_prob) for cell in fw_cells]\n",
    "    bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=keep_prob) for cell in bw_cells]\n",
    "\n",
    "rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "fw_cells, bw_cells, x_emb, sequence_length=x_len, dtype=tf.float32)\n",
    "\n",
    "with tf.name_scope(\"attention\"):\n",
    "    attention_score = tf.nn.softmax(tf.layers.dense(rnn_outputs, 1, activation=tf.nn.tanh), axis=1)\n",
    "    attention_out = tf.squeeze(\n",
    "    tf.matmul(tf.transpose(rnn_outputs, perm=[0, 2, 1]), attention_score),axis=-1)\n",
    "\n",
    "\n",
    "with tf.name_scope(\"output\"):\n",
    "    logits = tf.layers.dense(attention_out, num_class, activation=tf.nn.softmax)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()\n",
    "modelName = \"attentionRnn.ckpt-35125\"\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "saver.restore(sess, modelName)\n",
    "\n",
    "\n",
    "probabilities = sess.run(tf.nn.top_k(logits,2,sorted=True), feed_dict = {x: np.array(test_x[3]).reshape(1,50),is_training:False})\n",
    "print(\"부정: {}%\".format(probabilities[0][0][0]*100),\"긍정: {}%\".format(probabilities[0][0][1]*100))\n",
    "\n",
    "attention_weights = sess.run(attention_score,feed_dict = {x: np.array(test_x[3]).reshape(1,50),is_training:False})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
