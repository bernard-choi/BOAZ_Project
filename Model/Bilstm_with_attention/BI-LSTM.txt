 	num_class=30
        embedding_size = 256
        num_hidden = 256
        num_layers = 2
        learning_rate = 1e-3
        tf.reset_default_graph()

        x = tf.placeholder(tf.int32, [None, 80], name="x")
        x_len = tf.reduce_sum(tf.sign(x), 1)
        # 문장 길이 계산
        y = tf.placeholder(tf.int32, [None], name="y")
        is_training = tf.placeholder(tf.bool, [], name="is_training")
        #우리가 트레이닝 중인지 (phase=True), 아니면 테스팅 중인지(phase=False) 알려주는 변수입니다.
        #Batch Normalization이 트레이닝과 테스팅 시점에 서로 다르게 동작한다는 점을 기억해보세요.
        global_step = tf.Variable(0, trainable=False)
        keep_prob = tf.where(is_training, 0.5, 1.0)
        # training시dropout 비율을 0.5로 , test 시에는 dropout 안합니다.

        with tf.name_scope("embedding"):
            init_embeddings = tf.random_uniform([vocabulary_size, embedding_size])
            #모델링시 우리의 데이터에서나온 word_dict (모든 단어에 ID가 있음)전체 vocab size와
            #embedding size에 맞춰서 개별 단어ID의 행벡터의 초기값을 랜덤으로 설정 
            embeddings = tf.get_variable("embeddings", initializer=init_embeddings)
            x_emb = tf.nn.embedding_lookup(embeddings, x)
            # 그 후 문장에 맞춰서 우리가 지정한 단어벡터를 위에서 만든 룩업테이블에서 참조해서
            # 문장길이* 단어벡터 차원수의 벡터가 형성, 분류를 잘하는 방식으로 값 업데이트 

        with tf.name_scope("birnn"):
            fw_cells = [rnn.BasicLSTMCell(num_hidden) for _ in range(num_layers)]
            # input의 embedding 차원이 256이기 때문에 num_hidden만큼 업데이트 (예측)할 수 있음 
            bw_cells = [rnn.BasicLSTMCell(num_hidden) for _ in range(num_layers)]
            fw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=keep_prob) for cell in fw_cells]
            # dropout 0.5
            bw_cells = [rnn.DropoutWrapper(cell, output_keep_prob=keep_prob) for cell in bw_cells]

            rnn_outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(
            fw_cells, bw_cells, x_emb, sequence_length=x_len, dtype=tf.float32)
            #A tuple (outputs, output_state_fw, output_state_bw)

        with tf.name_scope("attention"):
            attention_score = tf.nn.softmax(tf.layers.dense(rnn_outputs, 1, activation=tf.nn.tanh), axis=1)
            attention_out = tf.squeeze(
                tf.matmul(tf.transpose(rnn_outputs, perm=[0, 2, 1]), attention_score),axis=-1)
            # bilstm에서 나온 결과값들(y1,y2..)와 context c를 받아서 vector z를 내는과정


        with tf.name_scope("output"):
            logits = tf.layers.dense(attention_out, num_class, 